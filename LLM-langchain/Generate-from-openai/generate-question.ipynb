{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the environment and import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv,os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_community.llms import ChatGLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生成一个列表，存储所有的可修复的产品的wiki标题，以及对于其内容的链接\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide_dir = r\"/root/autodl-tmp/fix-helper/data/Guide\"\n",
    "wiki_dir = r\"/root/autodl-tmp/fix-helper/data/Wiki\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "system_msg = \"\"\"\n",
    "You should simulate a customer's question whose device is broken, and an assistant's answer who help with repair device,\n",
    "for each input:\n",
    "firstly, simulate customer's question, make it informal, More divergent generation\n",
    "secondly, You need to based on the information and the question you generated just now to simulate the assitant's answer\n",
    "\n",
    "in every input , You will get a batch of repair methods or repair instructions from the repairman. \n",
    "\n",
    "Always Output in standard JSON parsed string , have 'question' in string and 'answer' in string :\n",
    "Before each output, check whether you have completed the above requirements\n",
    "\"\"\"\n",
    "prompt = \"\"\"\n",
    "I will give you a instruction and you should generate thing depend on it\n",
    "These instructions will be placed in the delimited symbols like ```` \n",
    "the instructions have a topic and content, I will give in below:\n",
    "\n",
    "````topic : {topic}````\n",
    "````content : {content}````\n",
    "\n",
    "Always Output in standard JSON parsed string , have 'question' in string and 'answer' in string :\n",
    "Before each output, check whether you have completed the above requirements\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import timeout_decorator\n",
    "# from func_timeout import func_set_timeout\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import time\n",
    "\n",
    "client = OpenAI(base_url='https://api.kwwai.top/v1', api_key='sk-VtFAumQUfwPCQJxe27379c076fE64e99B52e9078C87f6eD3')\n",
    "\n",
    "def get_completion_messages(text):\n",
    "    return [  \n",
    "        {\n",
    "            'role':'system', \n",
    "            'content': system_msg\n",
    "        },    \n",
    "        {\n",
    "            'role':'user', \n",
    "            'content': text\n",
    "        },  \n",
    "    ]\n",
    "\n",
    "# @timeout_decorator.timeout(65)  # 设置超时时间为65秒\n",
    "\n",
    "# @func_set_timeout(65)\n",
    "def get_completion_from_messages(\n",
    "    messages, \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    temperature=1.2, \n",
    "    max_tokens=3000,\n",
    "    retries = 3,\n",
    "):\n",
    "    retry_cnt = 0\n",
    "    backoff_time = 30\n",
    "    while retry_cnt <= retries:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=temperature, \n",
    "                max_tokens=max_tokens, \n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except:\n",
    "            # print(f\"OpenAIError: {e}.\")\n",
    "            # if \"Please reduce your prompt\" in str(e):\n",
    "            #     max_tokens = int(max_tokens * 0.8)\n",
    "            #     print(f\"Reducing target length to {max_tokens}, retrying...\")\n",
    "            # else:\n",
    "            #     print(f\"Retrying in {backoff_time} seconds...\")\n",
    "            #     time.sleep(backoff_time)\n",
    "            #     backoff_time *= 1.5\n",
    "            retry_cnt += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_in_file = json.dumps(json_data,ensure_ascii=False, indent=4)\n",
    "# with open('generate_file.json','w',encoding='utf-8') as file:\n",
    "#     file.write(json_in_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "guide_files = os.listdir(guide_dir)\n",
    "wiki_files = os.listdir(wiki_dir)\n",
    "guide_root = r\"/root/autodl-tmp/fix-helper/data/Guide\"\n",
    "wiki_root = r\"/root/autodl-tmp/fix-helper/data/Wiki\"\n",
    "generate_QA = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import urllib.parse\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "visited_file = set()\n",
    "\n",
    "def random_select_file(file_list):\n",
    "    select_file = random.choice(file_list)\n",
    "    while select_file in visited_file:\n",
    "        select_file = random.choice(file_list)\n",
    "    visited_file.add(select_file)\n",
    "    return select_file\n",
    "\n",
    "def get_single_completion(file_list,file_root):\n",
    "    file_src = random_select_file(file_list)\n",
    "    file_url = file_root + '/' + file_src\n",
    "    # print(file_url)\n",
    "    file_topic = file_src.split('.')[0]\n",
    "    file_topic = urllib.parse.unquote(file_topic)\n",
    "    try:\n",
    "        with open(file_url,'r',encoding='utf-8') as file:\n",
    "            file_content = file.read()\n",
    "    except:\n",
    "        return None\n",
    "    prompt_full = PromptTemplate.from_template(prompt).format_prompt(topic=file_topic,content=file_content)\n",
    "    message = get_completion_messages(prompt_full.text)\n",
    "    # print(message)\n",
    "    response = get_completion_from_messages(message)\n",
    "    return response\n",
    "\n",
    "def add_to_QA(file_list,file_root):\n",
    "    message = get_single_completion(file_list,file_root)\n",
    "    if message is None:\n",
    "        return\n",
    "    try:\n",
    "        item_one = json.loads(message)\n",
    "    except:\n",
    "        return\n",
    "    generate_QA.append(item_one)\n",
    "\n",
    "def do_generate():\n",
    "    for i in tqdm(range(80)):\n",
    "        add_to_QA(guide_files,guide_root)\n",
    "    for i in tqdm(range(20)):\n",
    "        add_to_QA(wiki_files,wiki_root)\n",
    "\n",
    "# do_generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = get_single_completion(guide_files,guide_root)\n",
    "# print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/80 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [04:46<00:00,  3.58s/it]\n",
      "100%|██████████| 20/20 [00:51<00:00,  2.55s/it]\n"
     ]
    }
   ],
   "source": [
    "do_generate()\n",
    "json_in_file = json.dumps(generate_QA,ensure_ascii=False, indent=4)\n",
    "with open('generate_file.json','w',encoding='utf-8') as file:\n",
    "    file.write(json_in_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_in_file = json.dumps(generate_QA,ensure_ascii=False, indent=4)\n",
    "with open('generate_file2.json','w',encoding='utf-8') as file:\n",
    "    file.write(json_in_file) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
